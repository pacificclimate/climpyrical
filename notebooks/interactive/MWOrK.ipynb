{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from climpyrical.gridding import scale_model_obs\n",
    "from climpyrical.mask import stratify_coords\n",
    "from climpyrical.data import read_data, interpolate_dataset, gen_dataset\n",
    "from climpyrical.rkrig import rkrig_r\n",
    "from climpyrical.cmd.find_matched_model_vals import add_model_values\n",
    "\n",
    "from pkg_resources import resource_filename\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters are loaded from config_example.yml\n",
    "Example configuration from config_example.yml:\n",
    "```\n",
    "    paths:\n",
    "        preprocessed_model_path: /data/results/intermediate/preprocessed_netcdf/\n",
    "    RL50:\n",
    "        station_dv: \"RL50 (kPa)\"\n",
    "        station_path: 'data/station_inputs/Interim_snow_rain_load_LR_composite_stations_tbd_v4.csv'\n",
    "        input_model_path: 'data/model_inputs/snw_rain_CanRCM4-LE_ens35_1951-2016_max_rl50_load_ensmean.nc'\n",
    "        medians: \n",
    "            value: 0.4\n",
    "            action: \"multiply\"\n",
    "        fill_glaciers: True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parameter cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load processed moded, target canada-only mask, and the Upper Artic Area mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard coded CanRCM4 upper model domain rlat\n",
    "dsold_max = 28.15999984741211\n",
    "\n",
    "ds = read_data(resource_filename(\"climpyrical\", f\"{preprocessed_model_path}{name}.nc\"))\n",
    "(dv, ) = ds.data_vars\n",
    "\n",
    "mask = read_data(\n",
    "    resource_filename(\n",
    "        'climpyrical',\n",
    "        mask_path\n",
    "    ))['mask'].values\n",
    "\n",
    "northern_mask = read_data(\n",
    "    resource_filename(\n",
    "        'climpyrical',\n",
    "        north_mask_path\n",
    "    )\n",
    ")['mask'].values\n",
    "\n",
    "rlon, rlat = np.meshgrid(ds.rlon, ds.rlat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load processed station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    resource_filename(\n",
    "        \"climpyrical\",\n",
    "        f\"{preprocessed_stations_path}{name}.csv\"\n",
    "    ), \n",
    "    index_col=False\n",
    ")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ds[dv].attrs[\"units\"] == \"degC\" and \"degC\" in station_dv:\n",
    "    K = 273.15\n",
    "    df[station_dv] += K\n",
    "    df[\"model_values\"] += K\n",
    "    ds[dv] += K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate stations into beyond and within the model domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_north = df[df.rlat > dsold_max].copy()\n",
    "df_south = df[df.rlat <= dsold_max].copy()\n",
    "\n",
    "north_index = df_north.index.values\n",
    "south_index = df_south.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for sets of stations that are identical. This would produce idential windows over a geographic area and disproportionately weight them in the averaged ratio field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_distances = np.stack([np.deg2rad(df_south.lat.values), np.deg2rad(df_south.lon.values)])\n",
    "nbrs = NearestNeighbors(n_neighbors=30, metric=\"haversine\").fit(\n",
    "    X_distances.T\n",
    ")\n",
    "\n",
    "# Order independent window checkers\n",
    "# only uses windows that are not-identical\n",
    "\n",
    "dist, ind = nbrs.kneighbors(X_distances.T)\n",
    "good_i = []\n",
    "list_of_sets = []\n",
    "count = 0 \n",
    "for i in range(df_south.shape[0]):\n",
    "    list_of_sets.append(df_south[[\"lon\", \"lat\", station_dv]].iloc[ind[i]].values)\n",
    "    if i+1-count == np.unique(list_of_sets, axis=0).shape[0]:\n",
    "        good_i.append(i)\n",
    "    else:\n",
    "        warning.warn(\"There are identical windows!\")\n",
    "        count += 1 \n",
    "\n",
    "df_south = df_south.iloc[good_i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the station average in the UAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UAA_station_mean = np.nanmean(df[station_dv][df.rlat > dsold_max-1])\n",
    "UAA_station_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the best tolerance scaling to scale the model values. The ratio is already automatically scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vals = df.model_values\n",
    "station_vals = df[station_dv]\n",
    "\n",
    "ratio, best_tol = scale_model_obs(df.model_values, station_vals)\n",
    "\n",
    "# apply correction\n",
    "model_vals_corrected = (model_vals/best_tol)\n",
    "mean_corrected = (ds[dv].values/best_tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform ratio kriging on the set of stations below the maximum CanRCM4 native domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_field = rkrig_r(df_south, 30, ds, station_dv)\n",
    "ratio_field[~mask] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the UAA strip to sample the reconstruction values from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = ~np.isnan(ds[dv].where((ds.lat > 72.) & (ds.lat < 73) & (ds.lon - 360 < -75) & (ds.lon - 360 > -127)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill and reconstruct the ratio field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nanmask = ~np.isnan(ratio_field)\n",
    "\n",
    "points = np.stack([rlon[nanmask], rlat[nanmask]]).T\n",
    "target_points = np.stack([rlon[nanmask^mask], rlat[nanmask^mask]]).T\n",
    "\n",
    "# We treat TJul2.5 and TwJul2.5 slightly differently than the other DVs\n",
    "# since an artefact appears in SW Yukon in the CanRCM4 models. Since\n",
    "# stations happen to be close to coastlines for this DV, we directly\n",
    "# fill with the nearest reconstructed value, as opposed to the ratio value for \n",
    "# the remaining DVs\n",
    "if station_dv == \"TJul2.5 (degC)\" or station_dv == \"TwJul2.5 (degC)\":\n",
    "    reconstructed_field = ratio_field*mean_corrected.copy()\n",
    "    target_values = reconstructed_field[nanmask]\n",
    "    reconstructed_field[nanmask^mask] = interpolate_dataset(points, target_values, target_points, 'nearest')\n",
    "else:\n",
    "    target_values = ratio_field[nanmask]\n",
    "    ratio_field[nanmask^mask] = interpolate_dataset(points, target_values, target_points, 'nearest')\n",
    "    reconstructed_field = ratio_field*mean_corrected.copy()\n",
    "\n",
    "reconstructed_field_strip_mean = np.nanmean(reconstructed_field[selection])\n",
    "combined_ratio_station_mean = np.mean([reconstructed_field_strip_mean, UAA_station_mean])\n",
    "reconstructed_field[northern_mask] = combined_ratio_station_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare kriged ratio values at station grids, with the original ratio values at station krids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4.5))\n",
    "\n",
    "violindata = np.concatenate([df.ratio, ratio_field[df.irlat, df.irlon]])\n",
    "\n",
    "b_str = [\"B\" for x in df.ratio.values]\n",
    "bp_str = [\"B'\" for x in ratio_field[df.irlat, df.irlon]]\n",
    "\n",
    "vstrings = np.concatenate([b_str, bp_str])\n",
    "vdf = pd.DataFrame({\"Ratios at Station Grid Cells (i)\": violindata, '': vstrings})\n",
    "\n",
    "ax.set_title(f\"{name} Distributions of Results\", fontsize=20)\n",
    "sns.violinplot(ax=ax, x=\"Ratios at Station Grid Cells (i)\", y='', data=vdf, palette=sns.color_palette('pastel'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize reconstruction stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Northern fill value:\"\n",
    "    \"\\n\"\n",
    "    \"Reconstruction\", reconstructed_field_strip_mean,\n",
    "    \"\\n\"\n",
    "    \"UAA_station_mean\", UAA_station_mean,\n",
    "    \"\\n\"\n",
    "    \"Combined\", combined_ratio_station_mean\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert back to degC if K was used in previous steps. Generate dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ds[dv].attrs[\"units\"] == \"degC\" and \"degC\" in station_dv:\n",
    "    print(\"Convert back to degC\")\n",
    "    K = 273.15 # K\n",
    "    reconstructed_field -= K\n",
    "    df[station_dv] -= K\n",
    "#     ds[dv].attrs[\"units\"] = \"degC\"\n",
    "\n",
    "ds_recon = gen_dataset(dv, reconstructed_field, ds.rlat, ds.rlon, ds.lat, ds.lon, unit=ds[dv].attrs[\"units\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in NBCC Tables for Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrc_path = resource_filename(\"climpyrical\", nbcc_loc_path)\n",
    "df_nrc = pd.read_excel(nrc_path).iloc[:-1]\n",
    "\n",
    "# fill problem values with better values from 2015\n",
    "id_typo = df_nrc[(df_nrc['2020 Longitude'] > 0) | (df_nrc['2020 Latitude'] < 40)].index\n",
    "df_nrc.loc[id_typo, '2020 Longitude'] = df_nrc['2015 Long.'].values[id_typo]\n",
    "df_nrc.loc[id_typo, '2020 Latitude'] = df_nrc['2015 Lat.'].values[id_typo]\n",
    "\n",
    "# process to get the model values\n",
    "df_nrc.rename(columns={\"2020 Longitude\": \"lon\",\n",
    "                      \"2020 Latitude\": \"lat\"})\n",
    "\n",
    "df_nrc_matched = pd.DataFrame(\n",
    "    {\n",
    "     'Location': df_nrc.Location, \n",
    "     'Prov': df_nrc.Prov,\n",
    "     \"2020 Elev (m)\": df_nrc[\"2020 Elev (m)\"],\n",
    "     'lon': df_nrc['2020 Longitude'], \n",
    "     'lat': df_nrc['2020 Latitude']\n",
    "     }\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This step aims to correct the final field to the NBCC 2015 results.__\n",
    "\n",
    "This is configurable by setting `nbcc_correction` to True or False.\n",
    "\n",
    "1) Find the median of these values. Compute the difference, d = median(R) - median(NRC), and ratio, f = median(R)/median(NRC) \n",
    "\n",
    "2) For the non-temperature DVs and HDD, apply the multiplicative correction (1/f)*R_j over the entire reconstruction grid. For the temperature DVs, apply the correction as R_j - d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = medians[\"value\"]\n",
    "action = medians[\"action\"]\n",
    "\n",
    "if action != \"add\" and action != \"multiply\" and action != \"None\":\n",
    "    raise ValueError(\"Please provide either add or multiply or None actions in config.\")\n",
    "\n",
    "    \n",
    "dfp = add_model_values(ds=ds_recon, df=df_nrc_matched)\n",
    "reconstructed_field_med = ds_recon[dv].values\n",
    "\n",
    "if nbcc_correction:\n",
    "    if value != \"None\" or action != \"None\":\n",
    "#         med_pcic = np.nanmedian(dfp[\"model_values\"])\n",
    "        med_pcic = np.nanmean(dfp[\"model_values\"])\n",
    "\n",
    "        if action == \"multiply\":\n",
    "            fr = med_pcic/value\n",
    "            reconstructed_field_med = (1/fr)*reconstructed_field_med\n",
    "            print(\"f:\", fr)\n",
    "        if action == \"add\":\n",
    "            d = med_pcic - value\n",
    "            reconstructed_field_med = reconstructed_field_med - d\n",
    "            print(\"d:\", d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-create dataset if median correction applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_recon = gen_dataset(dv, reconstructed_field_med, ds.rlat, ds.rlon, ds.lat, ds.lon, unit=ds[dv].attrs[\"units\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate with headers from the input processed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ds.attrs:\n",
    "    all_keys = set(ds_recon.variables).union(set(ds_recon.dims))\n",
    "    for key in all_keys:\n",
    "        ds_recon[key].attrs = ds[key].attrs\n",
    "    attr_dict = ds.attrs\n",
    "    attr_dict[\"Climpyrical\"] = (\n",
    "        \"CanRCM4 Reconstruction contains\"\n",
    "        \"hybrid station and model data using\"\n",
    "        \"Climpyrical (https://github.com/pacificclimate/climpyrical)\"\n",
    "    )\n",
    "\n",
    "    ds_recon.attrs = attr_dict\n",
    "else:\n",
    "    raise warnings.warn(\"No attributes detected in dataset file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_recon.to_netcdf(\n",
    "    resource_filename(\n",
    "        \"climpyrical\",\n",
    "        f\"{output_reconstruction_path}{name}_reconstruction.nc\"\n",
    "    ),\n",
    "    mode=\"w\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
